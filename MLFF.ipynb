{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFF(object):\n",
    "    def __init__(self,d1 = 27,weight_initializer = 'random'):\n",
    "        \"\"\"\n",
    "        a 3-layer MLFF network:  d input,  2*d+1 neuron in one hidden layer, 1 output neuron\n",
    "        \"\"\"\n",
    "        d2 = d1 * 2 + 1\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        # w1: the weights between input layer and hidden layer\n",
    "        # with shape of d2 rows x d1 columns.\n",
    "        # Each row of w1 represents the weigths associated with a neuron in hidden layer\n",
    "        # After adding bias, there are  d2*(d1+1) weights connecting input layer to hidden layer\n",
    "\n",
    "        if weight_initializer == 'random':\n",
    "            nums = np.round(np.random.random((d1+1) * d2)-np.random.random((d1+1) * d2), 4)*10\n",
    "        else:\n",
    "            nums = [0.0] * ((d1 + 1) * d2)\n",
    "        self.w1 = np.array(nums).reshape([d2,(d1+1)])\n",
    "\n",
    "        # w2: the weights between hidden layer and output layer\n",
    "        # there are d2 number of weight , plus one bias. Hence there are d2+1 weights\n",
    "        if weight_initializer == 'random':\n",
    "            nums = np.round(np.random.random(d2+1) - np.random.random(d2+1),3)*10\n",
    "        else:\n",
    "            nums = [0.0]*(d2+1)\n",
    "        self.w2 = np.array(nums)\n",
    "\n",
    "        self.activation_hidden = 'logistic'\n",
    "        self.activation_out = 'logistic'\n",
    "        # self.activate_h1 = lambda x:np.ones(self.d1)/(np.ones(self.d1)+ np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    fp = open('db4_Diff1f_100_128w_AEP2_gp2_ver1-1_ece856.txt','r+')\n",
    "    lines = fp.readlines()\n",
    "    input = []\n",
    "    target = []\n",
    "    for line in lines:\n",
    "        line = line.split('\\n')[0]\n",
    "\n",
    "        data=[float(s) for s in line.split(' ') if s!='']\n",
    "        input.append(data)\n",
    "        target.append(1)\n",
    "    fp.close()\n",
    "\n",
    "    fp = open('db4_Diff1f_100_128w_nonAEP2_gp2_ver1-1_ece856.txt', 'r+')\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.split('\\n')[0]\n",
    "        data=[float(s) for s in line.split(' ') if s != '']\n",
    "        input.append(data)\n",
    "        target.append(0)\n",
    "    fp.close()\n",
    "    print(\"shape\",np.shape(input),np.shape(target))\n",
    "    return np.array(target), np.array(input)\n",
    "\n",
    "\n",
    "def feedforward(i,model,bias = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param i: an input vector with shape 1x27\n",
    "    :param model: multi-layer feedforward network model\n",
    "    :param bias: a flag indicating if bias should be used\n",
    "    :return:\n",
    "    o1: intermmediate output from hidden layer\n",
    "     a scalar output o\n",
    "    \"\"\"\n",
    "    output = 0.0\n",
    "    input = np.zeros(len(i) + 1)\n",
    "    input[1:] = i\n",
    "    if bias:\n",
    "        input[0] = 1.0\n",
    "\n",
    "    # define logistic function\n",
    "    logistic = lambda net: np.ones(net.shape) / (np.ones(net.shape) + np.exp(-net))\n",
    "\n",
    "    input = np.transpose(input)\n",
    "    net_1= np.matmul(model.w1, input)\n",
    "    # activation\n",
    "    s1 =logistic(net_1)\n",
    "\n",
    "    # add feature x0 =1 to intermediate output if bias is enabled\n",
    "    o1= np.zeros(len(s1)+1)\n",
    "    o1[1:]= s1\n",
    "    if bias:\n",
    "        o1[0] = 1.0\n",
    "\n",
    "\n",
    "    # output layer\n",
    "    net_2 = np.dot(o1, model.w2)\n",
    "\n",
    "    # output of MLFF\n",
    "    if model.activation_out == 'logistic':\n",
    "        output = logistic(net_2)\n",
    "\n",
    "    return input, o1, output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backforward(o1,o2,t):\n",
    "    \"\"\"\n",
    "\n",
    "    :param o1:\n",
    "    :param o2:\n",
    "    :param t:\n",
    "    :param bias:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ## output layer:\n",
    "    # dE/do = (t-o)\n",
    "    diff = t-o2\n",
    "    dE_o2 = -diff\n",
    "    # do/dnet = o(1-o) for logistic function\n",
    "    dO_net = o2 * (1 - o2)\n",
    "\n",
    "    # dnet/w = o1\n",
    "    # delta = (t-o)*f'(net)= - dE/do * f'(net)\n",
    "    delta2 = -dE_o2 * dO_net\n",
    "\n",
    "    # hidden layer :\n",
    "    # Do not back-propagate bias in hidden layer , model.w2[0],  to the last layer\n",
    "    dE_o1 = -delta2 * model.w2[1:]\n",
    "    o1 = o1[1:]\n",
    "    dO1_net = o1 * (np.ones(np.shape(o1)) - o1)\n",
    "    delta1 = -dE_o1 * dO1_net\n",
    "\n",
    "\n",
    "    return delta1,delta2\n",
    "\n",
    "\n",
    "\n",
    "def predict(inputs,model):\n",
    "    pred = []\n",
    "    for i in inputs:\n",
    "        _,_, o= feedforward(i,model)\n",
    "        # print('o:',o)\n",
    "        pred.append(1 if o >= 0.5 else 0)\n",
    "\n",
    "    return  np.array(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GDR_train(inputs,targets,model,bias=False,alpha = 0.5 , beta = 0.5,momentum= False,max_iter= 1000):\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs:  a set of input vectors, i has shape nxm,  n is the number of sample,  m is the size of one sample\n",
    "    in this project, each sample has 27 features, hence m=27\n",
    "\n",
    "    :param targets:   a list of target with shape 1xn\n",
    "    :param model:   Multi-layer neural network model, containing weights, bias and other parameters. See class MLNN\n",
    "    :param bias:    a flag indicating if bias should be used\n",
    "    :param momentum:\n",
    "    :param alpha:  learning rate\n",
    "    :param beta: weight of momentum\n",
    "    :param max_iter:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    err_tss= []\n",
    "    sensitivity = []\n",
    "    l = len(inputs)\n",
    "    delta_wn1 = np.zeros(np.shape(model.w1))\n",
    "    delta_wn2 = np.zeros(np.shape(model.w2))\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "        err =0.0\n",
    "\n",
    "        # 1 epoch update\n",
    "        for i in range(l):\n",
    "            o0, o1, o2 =feedforward(inputs[i,:],model,bias)\n",
    "\n",
    "            diff =targets[i] - o2\n",
    "            err += np.square(diff)\n",
    "            # print('O:',o,'t:',targets[i])\n",
    "            # print(\"diff:\",diff)\n",
    "            # calculate Etss : Error = 0.5*(o-t)^2\n",
    "\n",
    "            # GDR update with backforward propagation\n",
    "            delta1, delta2 = backforward(o1,o2,targets[i])\n",
    "\n",
    "\n",
    "            # update weights in layer 2\n",
    "            delta_w2 = alpha * delta2 * o1\n",
    "            model.w2 += delta_w2\n",
    "            # update weights in layer 1\n",
    "            delta_w1 = alpha*np.reshape(delta1,[len(delta1),1])*o0.reshape([1,len(o0)])\n",
    "            model.w1 += delta_w1\n",
    "\n",
    "            if momentum:\n",
    "                model.w2 += beta * delta_wn2\n",
    "                model.w1 += beta * delta_wn1\n",
    "\n",
    "            delta_wn1 = delta_w1\n",
    "            delta_wn2 = delta_w2\n",
    "\n",
    "        err= 0.5*err\n",
    "        if epoch%50 ==0:\n",
    "            err_tss.append(err)\n",
    "\n",
    "            # Test sensitivity here\n",
    "            tot_postive = np.sum(targets)\n",
    "            pred = predict(inputs, model)\n",
    "            TP = np.sum(pred[np.where(targets == 1)])\n",
    "            # print('targets:',targets)\n",
    "            # print('pred:',pred)\n",
    "            sens = TP / tot_postive\n",
    "            sensitivity.append(sens)\n",
    "            print(\"Error: \",err,\"Sensitivity:\",sens)\n",
    "\n",
    "    return  err_tss,sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (2565, 27) (2565,)\n",
      "[[5.75649384e-01 3.15155736e-01 6.71845544e-01 ... 3.72115181e-02\n",
      "  7.86971160e-01 6.04854172e-01]\n",
      " [2.46205403e-02 9.83495078e-01 2.05158875e-02 ... 6.56734071e-03\n",
      "  1.00000000e+00 2.76393271e-01]\n",
      " [4.13829685e-03 9.97167297e-01 3.31908238e-03 ... 9.75580282e-04\n",
      "  1.60357320e-01 2.76393271e-01]\n",
      " ...\n",
      " [9.50166045e-02 8.98130426e-01 9.52653533e-02 ... 5.65209874e-03\n",
      "  1.00000000e+00 2.76393271e-01]\n",
      " [8.55194104e-03 9.88967757e-01 8.77022957e-03 ... 1.39856250e-02\n",
      "  1.60357320e-01 2.76393271e-01]\n",
      " [1.70506079e-02 9.81529039e-01 2.23492957e-02 ... 6.86669782e-03\n",
      "  1.00000000e+00 2.76393271e-01]] [0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1\n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
      " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0\n",
      " 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
      " 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0]\n",
      "Error:  31.556493286569385 Sensitivity: 0.3132530120481928\n",
      "Error:  8.76004465209209 Sensitivity: 0.6867469879518072\n",
      "Error:  6.365257944464253 Sensitivity: 0.43373493975903615\n",
      "Error:  5.834139334565099 Sensitivity: 0.4457831325301205\n",
      "Error:  3.5245882273600686 Sensitivity: 0.30120481927710846\n",
      "Error:  1.7609891860132232 Sensitivity: 0.6265060240963856\n",
      "Error:  2.2196164596630856 Sensitivity: 0.37349397590361444\n",
      "Error:  0.5479294923053278 Sensitivity: 0.7108433734939759\n",
      "Error:  0.5134892182648523 Sensitivity: 0.7590361445783133\n",
      "Error:  0.6268952224265083 Sensitivity: 0.6746987951807228\n",
      "Error:  0.5620506076705711 Sensitivity: 0.7951807228915663\n",
      "Error:  0.49673847771605745 Sensitivity: 0.7469879518072289\n",
      "Error:  1.5725582010099959 Sensitivity: 0.6385542168674698\n",
      "Error:  4.496886356463385 Sensitivity: 0.8433734939759037\n",
      "Error:  1.1844486547397697 Sensitivity: 0.5783132530120482\n",
      "Error:  2.6900407586901944 Sensitivity: 0.6987951807228916\n",
      "Error:  2.204111097705722 Sensitivity: 0.8313253012048193\n",
      "Error:  0.6593011189009006 Sensitivity: 0.927710843373494\n",
      "Error:  3.126492166583603 Sensitivity: 0.927710843373494\n",
      "Error:  1.0324371868683184 Sensitivity: 0.6024096385542169\n",
      "Error:  0.010923751426397784 Sensitivity: 0.7108433734939759\n",
      "Error:  0.007855379554453945 Sensitivity: 0.6987951807228916\n",
      "Error:  0.006483765360726853 Sensitivity: 0.6987951807228916\n",
      "Error:  0.005680324833144656 Sensitivity: 0.6987951807228916\n",
      "Error:  0.005158758363635324 Sensitivity: 0.7108433734939759\n",
      "Error:  0.004793681067460638 Sensitivity: 0.7108433734939759\n",
      "Error:  0.004520301867523347 Sensitivity: 0.7108433734939759\n",
      "Error:  0.004303274903864771 Sensitivity: 0.7228915662650602\n",
      "Error:  0.004122753250406008 Sensitivity: 0.7228915662650602\n",
      "Error:  0.003967254386790095 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0038298847194394264 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0037063222305179847 Sensitivity: 0.7228915662650602\n",
      "Error:  0.003593727267670238 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0034901443973419386 Sensitivity: 0.7108433734939759\n",
      "Error:  0.003394166081511298 Sensitivity: 0.7108433734939759\n",
      "Error:  0.0033047380433412882 Sensitivity: 0.7108433734939759\n",
      "Error:  0.0032210429364618602 Sensitivity: 0.7108433734939759\n",
      "Error:  0.003142428407655851 Sensitivity: 0.7108433734939759\n",
      "Error:  0.0030683610559260315 Sensitivity: 0.7108433734939759\n",
      "Error:  0.002998395959788643 Sensitivity: 0.7108433734939759\n",
      "Error:  0.0029321558543260406 Sensitivity: 0.7108433734939759\n",
      "Error:  0.0028693164724618208 Sensitivity: 0.7108433734939759\n",
      "Error:  0.0028095959398981556 Sensitivity: 0.7108433734939759\n",
      "Error:  0.002752746910104067 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0026985505996703777 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0026468121734560157 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0025973571097245665 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0025500282912420797 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0025046836441253145 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0024611941969646146 Sensitivity: 0.7228915662650602\n",
      "Error:  0.002419442467376618 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0023793211072231256 Sensitivity: 0.7228915662650602\n",
      "Error:  0.002340731754781701 Sensitivity: 0.7228915662650602\n",
      "Error:  0.002303584054410595 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0022677948132102527 Sensitivity: 0.7228915662650602\n",
      "Error:  0.002233287270813463 Sensitivity: 0.7228915662650602\n",
      "Error:  0.0021999904634201995 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0021678386669721048 Sensitivity: 0.7349397590361446\n",
      "Error:  0.002136770907275613 Sensitivity: 0.7349397590361446\n",
      "Error:  0.00210673052713844 Sensitivity: 0.7349397590361446\n",
      "Error:  0.002077664802359884 Sensitivity: 0.7349397590361446\n",
      "Error:  0.002049524599820302 Sensitivity: 0.7349397590361446\n",
      "Error:  0.002022264072037351 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0019958403834620217 Sensitivity: 0.7349397590361446\n",
      "Error:  0.001970213464521146 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0019453457900163267 Sensitivity: 0.7349397590361446\n",
      "Error:  0.001921202178980495 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0018977496135069959 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0018749570744073657 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0018527953918393667 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0018312371092907862 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0018102563595084695 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0017898287511385373 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0017699312649896624 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0017505421589645857 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0017316408808115164 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0017132079879456556 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0016952250736731354 Sensitivity: 0.7349397590361446\n",
      "Error:  0.001677674699223922 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0016605403310613566 Sensitivity: 0.7349397590361446\n",
      "Error:  0.0016438062829942693 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0016274576626655463 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0016114803220338304 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0015958608115044896 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0015805863373997862 Sensitivity: 0.7469879518072289\n",
      "Error:  0.001565644722486051 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0015510243693071346 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0015367142260909272 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0015227037550243562 Sensitivity: 0.7469879518072289\n",
      "Error:  0.001508982902704626 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0014955420725971656 Sensitivity: 0.7469879518072289\n",
      "Error:  0.00148237209934131 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0014694642247630831 Sensitivity: 0.7469879518072289\n",
      "Error:  0.001456810075463669 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0014444016418642916 Sensitivity: 0.7469879518072289\n",
      "Error:  0.001432231258599676 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0014202915861598578 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0014085755936884298 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0013970765428545128 Sensitivity: 0.7469879518072289\n",
      "Error:  0.0013857879727208083 Sensitivity: 0.7469879518072289\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAGz1JREFUeJzt3Xt0HOWZ5/Hv010ty7Jky7JkYXxBtrExt2CIuDgEEggwhswJsGE3w2YTZpaMM3vIDtnk7C7JnD3JnDM7k5ydCZOcZTNDgMQzk5BMuCxsLhDiOJAMjGMZjLGxsbgY3yX5LtnYUkvP/lElWZa7W9dWu0q/zzk66q6uVj3VJf/0+q233jJ3R0RE4i9V6gJERGRsKNBFRBJCgS4ikhAKdBGRhFCgi4gkhAJdRCQhFOgiIgmhQBcRSQgFuohIQgTjubHa2lpvaGgYz02KiMTeunXr9rl73WDrjWugNzQ00NTUNJ6bFBGJPTN7dyjrqctFRCQhFOgiIgmhQBcRSQgFuohIQijQRUQSQoEuIpIQCnQRkYSIRaCv2tzC//n1m6UuQ0TkjBaLQH9+axvfeeHtUpchInJGi0WgB6kUXd26mbWISCGxCPRMYHR195S6DBGRM1o8Aj2VUqCLiAwiHoGeTtHj0N2jbhcRkXxiEehB2gDUShcRKWDQQDezcjP7nZm9amabzOzPo+XzzWyNmTWb2Y/MrKxYRZalwzKzaqGLiOQ1lBb6CeB6d78EWAosN7OrgK8D97v7IuAgcHexiuxroWfVQhcRyWfQQPdQR/Q0E305cD3wWLR8JXBbUSok7EMH6OpRoIuI5DOkPnQzS5vZeqAVeA54Czjk7tlolZ3A7OKUCJm+PnR1uYiI5DOkQHf3bndfCswBrgDOz7Varvea2QozazKzpra2thEV2ddCV5eLiEhewxrl4u6HgF8DVwHVZtZ7T9I5wO4873nQ3RvdvbGubtB7nOYU9J0UVaCLiOQzlFEudWZWHT2eDNwAbAZWA3dEq90FPFWsIsuiLpfOrLpcRETyCQZfhVnASjNLE/4B+Gd3/4mZvQ780Mz+AngFeLhoRabUQhcRGcygge7uG4BLcyx/m7A/vegyQdSHrguLRETyisWVopmURrmIiAwmHoGuFrqIyKBiEehB1ELPqoUuIpJXLAK9dxx6p1roIiJ5xSrQ1UIXEckvJoGu6XNFRAYTk0DXSVERkcHELNDV5SIikk8sAr13PnRdKSoikl8sAr1vlItmWxQRySsmgd7bQleXi4hIPjEJdM2HLiIymFgEeu+Vol1qoYuI5BWLQDczMmnTsEURkQJiEegQzomeVaCLiOQVm0APW+jqchERySc2gV4WpNTlIiJSQGwCPUgp0EVEColNoGcC02yLIiIFxCfQUynNhy4iUkB8Aj2dUgtdRKSA2AR6oHHoIiIFxSbQM+mUrhQVESlg0EA3s7lmttrMNpvZJjO7N1r+VTPbZWbro69billoJm2ay0VEpIBgCOtkgS+6+8tmVgWsM7Pnotfud/e/Ll55J2XSGrYoIlLIoIHu7nuAPdHjdjPbDMwudmEDBekURzu7x3uzIiKxMaw+dDNrAC4F1kSLPmdmG8zsETObnuc9K8ysycya2traRlxoWdo0l4uISAFDDnQzqwQeBz7v7keAbwMLgaWELfi/yfU+d3/Q3RvdvbGurm7EhepKURGRwoYU6GaWIQzz77v7EwDu3uLu3e7eA3wHuKJ4ZUIm0Dh0EZFChjLKxYCHgc3u/o1+y2f1W+12YOPYl3dSJmW6UlREpIChjHK5GvgU8JqZrY+WfRm408yWAg5sAz5blAojulJURKSwoYxy+S1gOV762diXk5+uFBURKSxeV4oq0EVE8opRoOuORSIihcQo0NVCFxEpJDaBHqRTZHscd7XSRURyiU2gl6XD87LqdhERyS02gR6kw1KzPep2ERHJJTaBnokCvSurFrqISC4xCvSoy0UtdBGRnGIU6FELXSNdRERyik2gB6mwha7L/0VEcotNoJcFYamaoEtEJLfYBHqQika5qIUuIpJTbAK976SoWugiIjnFKNB1UlREpJAYBrq6XEREcolNoAfp3lEuaqGLiOQSm0DvbaFrlIuISG4xCnSNQxcRKSRGga6ToiIihcQo0HvnclELXUQklxgFeu9si2qhi4jkEptA13zoIiKFDRroZjbXzFab2WYz22Rm90bLa8zsOTNrjr5PL2ahvV0unTopKiKS01Ba6Fngi+5+PnAVcI+ZXQDcB6xy90XAquh50WT65nJRC11EJJdBA93d97j7y9HjdmAzMBu4FVgZrbYSuK1YRQJkAo1yEREpZFh96GbWAFwKrAHq3X0PhKEPzBzr4vrrnQ9dl/6LiOQ25EA3s0rgceDz7n5kGO9bYWZNZtbU1tY2khoBjUMXERnMkALdzDKEYf59d38iWtxiZrOi12cBrbne6+4PunujuzfW1dWNuNB0ykiZrhQVEclnKKNcDHgY2Ozu3+j30tPAXdHju4Cnxr68U2XSKbXQRUTyCIawztXAp4DXzGx9tOzLwNeAfzazu4HtwL8tToknhYGuFrqISC6DBrq7/xawPC9/ZGzLKSyTNrXQRUTyiM2VohBeLaorRUVEcotVoJelU3Rm1eUiIpJLrAI9kza10EVE8ohVoAca5SIiklesAl2jXERE8otZoGuUi4hIPjEL9JSuFBURySNWgR6kjE610EVEcopVoJcFKc2HLiKSR6wCPUiZToqKiOQRq0DX5FwiIvkp0EVEEiJmga4uFxGRfGIV6EFaJ0VFRPKJVaBn0ik61UIXEckpZoGuyblERPKJWaCn6Moq0EVEcolVoAdpo6tHXS4iIrnEKtDLNGxRRCSvWAV6kErhDt1qpYuInCZWgZ4JwntVq5UuInK6eAV6KixXgS4icrpBA93MHjGzVjPb2G/ZV81sl5mtj75uKW6ZoUy6t4WuLhcRkYGG0kL/HrA8x/L73X1p9PWzsS0rtyAdlqurRUVETjdooLv7C8CBcahlUGVRoOsmFyIipxtNH/rnzGxD1CUzfcwqKiCIulx0GzoRkdONNNC/DSwElgJ7gL/Jt6KZrTCzJjNramtrG+HmQpm0ToqKiOQzokB39xZ373b3HuA7wBUF1n3Q3RvdvbGurm6kdQI6KSoiUsiIAt3MZvV7ejuwMd+6Y0ktdBGR/ILBVjCzR4EPA7VmthP4CvBhM1sKOLAN+GwRa+zTN8pFMy6KiJxm0EB39ztzLH64CLUMqrfLpTOrLhcRkYHidaWoWugiInnFMtDVhy4icrpYBXqQ0igXEZF8YhXoZYFa6CIi+cQq0Htb6LpSVETkdLEK9IzmchERySuWga4WuojI6WIW6LpjkYhIPrEK9EDDFkVE8opVoJf1Bbq6XEREBopVoJ+cD10tdBGRgeIV6Cn1oYuI5BOrQDczMmmjq0ddLiIiA8Uq0CEcutiVVQtdRGSg2AV6kDKyaqGLiJwmdoFeFqR0paiISA6xC/QgldIoFxGRHGIX6JnANA5dRCSH+AV6KqVhiyIiOcQv0NMKdBGRXOIX6OpyERHJKXaBHqjLRUQkp9gFepm6XEREcho00M3sETNrNbON/ZbVmNlzZtYcfZ9e3DJPCtKmG1yIiOQwlBb694DlA5bdB6xy90XAquj5uNBJURGR3AYNdHd/ATgwYPGtwMro8UrgtjGuK69MWidFRURyGWkfer277wGIvs/Mt6KZrTCzJjNramtrG+HmTlILXUQkt6KfFHX3B9290d0b6+rqRv3zgnRKk3OJiOQw0kBvMbNZANH31rErqbBM2ujU9LkiIqcZaaA/DdwVPb4LeGpsyhlcJpUi26NAFxEZaCjDFh8FXgLOM7OdZnY38DXgRjNrBm6Mno8LXSkqIpJbMNgK7n5nnpc+Msa1DImuFBURyS1+V4oGCnQRkVxiF+hBSl0uIiK5xC7Q66om0d3jtBw5XupSRETOKLEL9PfNmQbAhp2HS1yJiMiZJXaBfsGsaaQMNuw8VOpSRETOKLEL9MllaRbXV6mFLiIyQOwCHcJul9d2HcZdJ0dFRHrFMtAvnlPNgaOd7Dz4XqlLERE5Y8Qy0C+JToy+tkvdLiIivWIZ6OedVUUmbbyqE6MiIn1iGeiTgjTnz5rKazoxKiLSJ5aBDnDx7PDEaI/mRhcRAWIc6JfMqab9eJZt+4+WuhQRkTNCbAP9Yp0YFRE5RWwDfdHMSsozKV7doUAXEYEYB3qQTnHh2dN4bZdGuoiIQIwDHcIToxt3HeHoiWypSxERKblYB/otF8+is7uHP/mndZzIdpe6HBGRkop1oF8xv4avf/x9/KZ5H1/40at0awijiExgg95T9Ex3x/vncOhYJ3/x081MnZzhL2+/CDMrdVkiIuMu1i30Xp+5ZgF/fM18Hv3ddt5oaS91OSIiJZGIQAf45JXnAPDKdo16EZGJaVSBbmbbzOw1M1tvZk1jVdRInDOjguqKDOsV6CIyQY1FH/p17r5vDH7OqJgZl8yp1gyMIjJhJabLBeCSudVsbWnXuHQRmZBGG+gO/MLM1pnZirEoaDQunVtNj2t+FxGZmEYb6Fe7+2XAzcA9ZnbtwBXMbIWZNZlZU1tb2yg3V9j7ogm7Xt2hbhcRmXhGFejuvjv63go8CVyRY50H3b3R3Rvr6upGs7lBzaicxLyaCtYr0EVkAhpxoJvZFDOr6n0M3ARsHKvCRuqSudXj3kL/++ff4j9+b+24blNEZKDRtNDrgd+a2avA74CfuvszY1PWyC2dW83uw8dpPXJ83La5aksrz29tozPbM27bFBEZaMTDFt39beCSMaxlTCydG/ajr99xiJsuPKvo23N3mlva6e5xdhw8xsK6yqJvU0Qkl0QNWwS48OxpBCkbt/Ho+zo6OXisC4B32nQ7PBEpncQFenkmzZJZVeN2YrS539wx7+xToItI6SQu0CG8gfSGHYfpGYfpdLdGgV6WTvG2Al1ESiiRgb50bjXtJ7K81dZR9G1tbe1gannAhbOnsk2BLiIllMhAv3L+DAD+5c3iTzHT3NLO4voqFtRWqstFREoqkYE+b0YFDTMqeKG5uIHu7mxt6WBRfRUL6qaw98jxYc8j09Pj/PL1lnHpHhKRZEtkoANcu7iOl97aX9R7jba1n+Dwe10srq9kfu0UALbtH14r/fnmNj7zD038cnNLMUoUkQkksYH+ocV1vNfVTdO2g0XbxtaWsI9+cX1VX6APt9tlw45wIrG12w6MbXEiMuEkNtCvWjCDTNp4YWvxJgTrHeGyqL6ShhlRoPcbi/5maztf+/kW3uvM/7+E3pkh1xbxD4+ITAyJDfQpkwIaz6nh+SIGenNrO9UVGeoqJzG5LM3Z08pPaaF/a9Wb/F00z8uxztx96xujQN+463DB4BcRGUxiAx3CfvQte9tpKdK8LltbOlg8swozA2B+3ZS+sejHu7r51ZZWlpxVxZp39vNH3z091NvaT7D3yHGuPncG2R7XLJEiMioJD/RagL5ul5Yjx/nSExvYceDYqH92OMKlnUX1J+dumV87hbfbOnB3ftu8j44TWe67eQn3f2Ipa7cd4I++u5Zs98kJvHpb53cta8AMmtSPLiKjMBb3FD1jnX/WVGorJ/FC8z6umF/Df3h4DTsOvMfU8gxfuuX8Uf3sliMnaD+eZXF9Vd+y+bWVHDme5eCxLn6+cS9V5QEfWFhLWZCi40SWP3tyI//69gE+uCj8Q9Pbf75s4QzOq69i7bvqRxeRkUt0Cz2VMq5dXMvzb7Ryx9+9RPvxLItmVvKrLa2j/tn9T4j2ml9b0ffac6/v5cYL6ikLwo/445fNYXImzTOb9vSt/9quwyyonUJVeYbGhum8/O5BujUeXURGKNGBDuHwxSPHs6QMfvzZZXzi8rk0t3aMutulN9AHttABfrBmO0eOZ7n5oll9r5Vn0nxocR2/2HTyIqKNuw5z0exwut/LG2roOJFly94jo6pLRCauxAf6TRecxZ9efy6P/ckHWFRfxfVLZgKw+o3RtdK3trRTM6WM2spJfcvmTJ9MkDJ+smE3U8rSXBN1rfRaftFZtLafYP3OQ+zrOMGew8e5OAr0xoYagKKOmxeRZEt8oE8uS/OFm85jbk3YHbKgrpKGGRWj6nY51pnlF6+3cHnD9FOWZ9Ip5tVU0ONw/fn1lGfSp7x+3ZKZBCnj2U17+06I9rbQZ1dP5uxp5brASERGLPGBnst1S2by4lv7844NH8xj63Zy6FgXn7lmwWmv9V4xestFp98tadrkDMsWzuDZjScD/cLZU/teb2yoYe22A7irH11Ehm9CBvr1S2bSme3hxTf3D/u93T3OQ795h0vnVdN4zvTTXr/g7KlUlQd86Ly6nO9fftFZbNt/jCdf2cX82ilMLc/0vXZ5w3Rajpxg58H3hl2XiEiihy3mc8X8GirK0vzqjVZuuKB+WO99ZuNeth84xpdvWdJ3QRE93eA9kM5wz3Xn8u+vnEdFWY6P9uh+PlqxmZ3B0yw8tJt50yfBEz+AoByW3cOVC84G4NlNe3O2/kVEColfoPf0QGcHlE8dfN08JgXhCcs9r7+Iz9+IdeyFjjaYUgszL4CZS+DIbtj2L7D9RciegMp6vLKe7RuNG6rP5cYltdC+F9athHXfhc6jcPW9lF/1n5g1Lex24UQ7vPsivLUa3l4NbVuoBv57AHu8himdFbAjgKP74NUfsvi6L3FVw+U89Jt3+NRl05nU9jr0ZCFdFn71/gE5hYefSfcJ6O4M/7Dk+9y6O8OvnlFOMZAOBqmpSNxP7kP3yLrLisr7HYczsb4zhXeH/6a6u8LHE8X7PgEzFhZ1Ezae/bWNjY3e1NQ08h/Q3gKP3w071sD7/xCu+SJUnd5XfYrta2DtQ7DoJrjo45BKwfHDvPNP/5n5O5/qW82DyVj21K4OxzhYtYjU5GlMObGP1NEW0tlouGOmIgrHLCz8CKQzsPUZqDwLzv992LUO9rwa/iMPymHeMljwITj7Mla+M42vPLeLH/zxlXxgYS10tMJPvwCb/x8d1eex88AxzkvtxFBfukhifPJxWHTDiN5qZuvcvXHQ9UYT6Ga2HPgmkAYecvevFVp/VIH+zm/CMD9+BBbfBFt+CqkMXPzxk+GaysD8a2DBdZAKYPX/hJceCMO2uzNsfV/2aXjpAfzIbh7IfozHsh+k1adzjHKmcpRzbRdXT23jkE3jmY4FtGUr+hXhXFhxmCc/lqFszzoIJsFld538q/vuS/DLr8DuV2B2IzRcDedcHYZ5przvp3ScyPLjph18elkD6VTUwnWHTU/iL/wvXj44mfUs5g/vuJ30pMqTrdJ8xyoVhPuYLoNUOvc6ljrZqk6lgZG2rD1sWXV3ht/H9Y+OQdC7DwEj34ciMQtrCyZF9UlOlgo/o0K/r3KKoge6maWBrcCNwE5gLXCnu7+e7z0jDvQX/zc89z+gZiH8u5VQfyEceBt+/XV44+dhqztdFnZ7dHaEwT65Go62QePdcMNX4M1fwuq/gv3NMONcuP3vac6cx7v7j3HovS4Ov9fFvJoK3n/OdGqmlAHh3YRa2o+z48B7bD9wjO0HjtF4znSuXZz7hGefnp6wphF6dtNePvuP6/jbTyzltktnj/jniEgyjEegLwO+6u6/Fz3/EoC7/1W+94w40Nf/AJqfg499CyZV5V+vOxt2xzQ/C61bYNk9YTdH/9d3vxL+QSiryP9zSqynx1n+zRdwh2c/fy2p1BnWEhWRcTUegX4HsNzdPxM9/xRwpbt/Lt97RtXl4j6+J+BK7Kn1u7j3h+upq5pEkDJS/fbd7ORHYdgpH0uhT8iG+fmN6tMu4aFKwm/JcI+VjL/hHqG//DcXc3l0RfiwtzXEQB9NR1+eIRenFbICWAEwb968UWxtYv2Cf/TiWWxtaWdfeyc97vTO2eV436fscMpFSIX+NA/37/ZoesZLeWFUIk4jJ2Inks1HcJAmZ4p/vmA0gb4TmNvv+Rxg98CV3P1B4EEIW+ij2N6EEqRT/NffW1LqMkQkRkZzpehaYJGZzTezMuAPgKfHpiwRERmuEbfQ3T1rZp8DniUctviIu28as8pERGRYRjVY1t1/BvxsjGoREZFRmJCTc4mIJJECXUQkIRToIiIJoUAXEUkIBbqISEKM6/S5ZtYGvDvCt9cC+8awnLiYiPs9EfcZJuZ+T8R9huHv9znuPsisgOMc6KNhZk1DmcsgaSbifk/EfYaJud8TcZ+hePutLhcRkYRQoIuIJEScAv3BUhdQIhNxvyfiPsPE3O+JuM9QpP2OTR+6iIgUFqcWuoiIFBCLQDez5Wb2hpm9aWb3lbqeYjCzuWa22sw2m9kmM7s3Wl5jZs+ZWXP0fXqpax1rZpY2s1fM7CfR8/lmtiba5x9F0zMniplVm9ljZrYlOubLkn6szey/RL/bG83sUTMrT+KxNrNHzKzVzDb2W5bz2FroW1G2bTCzy0az7TM+0KObUT8A3AxcANxpZheUtqqiyAJfdPfzgauAe6L9vA9Y5e6LgFXR86S5F9jc7/nXgfujfT4I3F2Sqorrm8Az7r4EuIRw/xN7rM1sNvCnQKO7X0Q45fYfkMxj/T1g+YBl+Y7tzcCi6GsF8O3RbPiMD3TgCuBNd3/b3TuBHwK3lrimMefue9z95ehxO+E/8NmE+7oyWm0lcFtpKiwOM5sDfBR4KHpuwPXAY9EqSdznqcC1wMMA7t7p7odI+LEmnK57spkFQAWwhwQea3d/ATgwYHG+Y3sr8A8e+leg2sxmjXTbcQj02cCOfs93RssSy8wagEuBNUC9u++BMPSBmaWrrCj+FvhvQE/0fAZwyN2z0fMkHu8FQBvw3air6SEzm0KCj7W77wL+GthOGOSHgXUk/1j3yndsxzTf4hDoQ7oZdVKYWSXwOPB5dz9S6nqKycx+H2h193X9F+dYNWnHOwAuA77t7pcCR0lQ90ouUZ/xrcB84GxgCmF3w0BJO9aDGdPf9zgE+pBuRp0EZpYhDPPvu/sT0eKW3v+CRd9bS1VfEVwNfMzMthF2pV1P2GKvjv5bDsk83juBne6+Jnr+GGHAJ/lY3wC84+5t7t4FPAF8gOQf6175ju2Y5lscAn1C3Iw66jt+GNjs7t/o99LTwF3R47uAp8a7tmJx9y+5+xx3byA8rr9y908Cq4E7otUStc8A7r4X2GFm50WLPgK8ToKPNWFXy1VmVhH9rvfuc6KPdT/5ju3TwKej0S5XAYd7u2ZGxN3P+C/gFmAr8BbwZ6Wup0j7+EHC/2ptANZHX7cQ9imvApqj7zWlrrVI+/9h4CfR4wXA74A3gR8Dk0pdXxH2dynQFB3v/wtMT/qxBv4c2AJsBP4RmJTEYw08SnieoIuwBX53vmNL2OXyQJRtrxGOAhrxtnWlqIhIQsShy0VERIZAgS4ikhAKdBGRhFCgi4gkhAJdRCQhFOgiIgmhQBcRSQgFuohIQvx/B3bX3HIIYVAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "t, x = load_data()\n",
    "#normalize data\n",
    "x = (x- np.min(x,axis=0))/(np.max(x,axis=0) - np.min(x,axis=0))\n",
    "train_x = np.zeros([200,27])\n",
    "train_x[0:100,:] = x[0:100,:]\n",
    "train_x[100:, :] = x[200:300, :]\n",
    "train_y=np.array([0]*200)\n",
    "train_y[0:100] = t[0:100]\n",
    "train_y[100:] = t[200:300]\n",
    "# i,t = shuffle(x,t)\n",
    "train_x, train_y = shuffle(train_x, train_y)\n",
    "print(train_x,train_y)\n",
    "\n",
    "#balance data\n",
    "# construct d x 2d+1 x 1 MFLL,  d= 27. Hence 3-layer network is 27x55x1\n",
    "# The activation functions in the hidden layer are logistic function\n",
    "model = MLFF(weight_initializer='random')\n",
    "\n",
    "    # o1, o = feedforward(i[1,:],model)\n",
    "err_tss, sensitivity= GDR_train(train_x,train_y,model, bias=True,alpha=0.6,beta=0.6, momentum=False,max_iter=5000)\n",
    "epochs = [i for i in range(len(err_tss))]\n",
    "plt.plot(epochs,err_tss)\n",
    "plt.plot(epochs, sensitivity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.plot(epochs, sensitivity)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(epochs, err_tss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  43.627929108963286 Sensitivity: 0.012048192771084338\n",
      "Error:  41.500277010099055 Sensitivity: 0.0\n",
      "Error:  41.50013084068338 Sensitivity: 0.0\n",
      "Error:  41.50008530672728 Sensitivity: 0.0\n",
      "Error:  41.50006316862458 Sensitivity: 0.0\n",
      "Error:  41.500050101114795 Sensitivity: 0.0\n",
      "Error:  41.50004148535976 Sensitivity: 0.0\n",
      "Error:  41.50003538128776 Sensitivity: 0.0\n",
      "Error:  41.50003083226789 Sensitivity: 0.0\n",
      "Error:  41.500027312330715 Sensitivity: 0.0\n",
      "Error:  41.50002450845642 Sensitivity: 0.0\n",
      "Error:  41.50002222276615 Sensitivity: 0.0\n",
      "Error:  41.50002032406837 Sensitivity: 0.0\n",
      "Error:  41.5000187219653 Sensitivity: 0.0\n",
      "Error:  41.50001735215015 Sensitivity: 0.0\n",
      "Error:  41.500016167633355 Sensitivity: 0.0\n",
      "Error:  41.50001513328481 Sensitivity: 0.0\n",
      "Error:  41.500014222316885 Sensitivity: 0.0\n",
      "Error:  41.50001341394839 Sensitivity: 0.0\n",
      "Error:  41.50001269181165 Sensitivity: 0.0\n",
      "Error:  41.500012042840694 Sensitivity: 0.0\n",
      "Error:  41.500011456479186 Sensitivity: 0.0\n",
      "Error:  41.50001092410593 Sensitivity: 0.0\n",
      "Error:  41.50001043861125 Sensitivity: 0.0\n",
      "Error:  41.500009994079846 Sensitivity: 0.0\n",
      "Error:  41.50000958555022 Sensitivity: 0.0\n",
      "Error:  41.50000920883003 Sensitivity: 0.0\n",
      "Error:  41.5000088603528 Sensitivity: 0.0\n",
      "Error:  41.50000853706533 Sensitivity: 0.0\n",
      "Error:  41.50000823633861 Sensitivity: 0.0\n",
      "Error:  41.500007955896784 Sensitivity: 0.0\n",
      "Error:  41.500007693759684 Sensitivity: 0.0\n",
      "Error:  41.50000744819655 Sensitivity: 0.0\n",
      "Error:  41.500007217687745 Sensitivity: 0.0\n",
      "Error:  41.50000700089373 Sensitivity: 0.0\n",
      "Error:  41.50000679662916 Sensitivity: 0.0\n",
      "Error:  41.50000660384115 Sensitivity: 0.0\n",
      "Error:  41.500006421591564 Sensitivity: 0.0\n",
      "Error:  41.50000624904156 Sensitivity: 0.0\n",
      "Error:  41.50000608543893 Sensitivity: 0.0\n",
      "Error:  41.5000059301073 Sensitivity: 0.0\n",
      "Error:  41.50000578243653 Sensitivity: 0.0\n",
      "Error:  41.500005641875084 Sensitivity: 0.0\n",
      "Error:  41.50000550792301 Sensitivity: 0.0\n",
      "Error:  41.50000538012614 Sensitivity: 0.0\n",
      "Error:  41.50000525807093 Sensitivity: 0.0\n",
      "Error:  41.500005141379965 Sensitivity: 0.0\n",
      "Error:  41.50000502970819 Sensitivity: 0.0\n",
      "Error:  41.500004922739514 Sensitivity: 0.0\n",
      "Error:  41.500004820183754 Sensitivity: 0.0\n",
      "Error:  41.500004721774104 Sensitivity: 0.0\n",
      "Error:  41.50000462726485 Sensitivity: 0.0\n",
      "Error:  41.50000453642924 Sensitivity: 0.0\n",
      "Error:  41.500004449057805 Sensitivity: 0.0\n",
      "Error:  41.50000436495656 Sensitivity: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2574e263479d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# o1, o = feedforward(i[1,:],model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merr_tss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitivity\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mGDR_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_tss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fddf2b6f0178>\u001b[0m in \u001b[0;36mGDR_train\u001b[0;34m(inputs, targets, model, bias, alpha, beta, momentum, max_iter)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 1 epoch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mo0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mo2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-50e5cc22dd1f>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(i, model, bias)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mnet_1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# add feature x0 =1 to intermediate output if bias is enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-50e5cc22dd1f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# define logistic function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2 = MLFF(weight_initializer='random')\n",
    "\n",
    "# o1, o = feedforward(i[1,:],model)\n",
    "err_tss, sensitivity= GDR_train(train_x,train_y,model2, bias=True,alpha=0.6,beta=0.6, momentum=True,max_iter=5000)\n",
    "epochs = [i for i in range(len(err_tss))]\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(epochs, sensitivity)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(epochs, err_tss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLFF(object):\n",
    "    def __init__(self,d1 = 27,weight_initializer = 'random'):\n",
    "        \"\"\"\n",
    "        a 3-layer MLFF network:  d input,  2*d+1 neuron in one hidden layer, 1 output neuron\n",
    "        \"\"\"\n",
    "        d2 = d1 * 2 + 1\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "        # list of activation function\n",
    "        self.activate = [None]\n",
    "        # list of activation function derivative\n",
    "        self.derivative=[None]\n",
    "        # Layer 1\n",
    "        # w1: the weights between input layer and hidden layer\n",
    "        # with shape of d2 rows x d1 columns.\n",
    "        # Each row of w1 represents the weigths associated with a neuron in hidden layer\n",
    "        # After adding bias, there are  d2*(d1+1) weights connecting input layer to hidden layer\n",
    "        if weight_initializer == 'random':\n",
    "            nums = np.round(np.random.random((d1+1) * d2)-np.random.random((d1+1) * d2), 4)*10\n",
    "        else:\n",
    "            nums = [0.0] * ((d1 + 1) * d2)\n",
    "        self.w1 = np.array(nums).reshape([d2,(d1+1)])\n",
    "        self.activate.append(self.logistic)\n",
    "        self.derivative.append(self.derivative_logistic)\n",
    "\n",
    "        # Layer 2:\n",
    "        # w2: the weights between hidden layer and output layer\n",
    "        # there are d2 number of weight , plus one bias. Hence there are d2+1 weights\n",
    "        if weight_initializer == 'random':\n",
    "            nums = np.round(np.random.random(d2+1) - np.random.random(d2+1),3)*10\n",
    "        else:\n",
    "            nums = [0.0]*(d2+1)\n",
    "        self.w2 = np.array(nums)\n",
    "        self.activate.append(self.logistic)\n",
    "        self.derivative.append(self.derivative_logistic)\n",
    "\n",
    "        # self.activate_h1 = lambda x:np.ones(self.d1)/(np.ones(self.d1)+ np.exp(-x))\n",
    "\n",
    "\n",
    "    def logistic(self,net):\n",
    "        return  np.ones(net.shape) / (np.ones(net.shape) + np.exp(-net))\n",
    "    def derivative_logistic(self,o):\n",
    "        return  o * (np.ones(np.shape(o)) - o)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    fp = open('db4_Diff1f_100_128w_AEP2_gp2_ver1-1_ece856.txt','r+')\n",
    "    lines = fp.readlines()\n",
    "    input = []\n",
    "    target = []\n",
    "    for line in lines:\n",
    "        line = line.split('\\n')[0]\n",
    "\n",
    "        data=[float(s) for s in line.split(' ') if s!='']\n",
    "        input.append(data)\n",
    "        target.append(1)\n",
    "    fp.close()\n",
    "\n",
    "    fp = open('db4_Diff1f_100_128w_nonAEP2_gp2_ver1-1_ece856.txt', 'r+')\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.split('\\n')[0]\n",
    "        data=[float(s) for s in line.split(' ') if s != '']\n",
    "        input.append(data)\n",
    "        target.append(0)\n",
    "    fp.close()\n",
    "    print(\"shape\",np.shape(input),np.shape(target))\n",
    "    return np.array(target), np.array(input)\n",
    "\n",
    "\n",
    "def feedforward(i,model,bias = False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param i: an input vector with shape 1x27\n",
    "    :param model: multi-layer feedforward network model\n",
    "    :param bias: a flag indicating if bias should be used\n",
    "    :return:\n",
    "    o1: intermmediate output from hidden layer\n",
    "     a scalar output o\n",
    "    \"\"\"\n",
    "    output = 0.0\n",
    "    input = np.zeros(len(i) + 1)\n",
    "    input[1:] = i\n",
    "    if bias:\n",
    "        input[0] = 1.0\n",
    "\n",
    "\n",
    "    input = np.transpose(input)\n",
    "    net_1= np.matmul(model.w1, input)\n",
    "    # activation of hidden layer\n",
    "    s1 =model.activate[1](net_1)\n",
    "\n",
    "    # add feature x0 =1 to intermediate output if bias is enabled\n",
    "    o1= np.zeros(len(s1)+1)\n",
    "    o1[1:]= s1\n",
    "    if bias:\n",
    "        o1[0] = 1.0\n",
    "\n",
    "    # output layer\n",
    "    net_2 = np.dot(o1, model.w2)\n",
    "    # output of MLFF\n",
    "    output = model.activate[2](net_2)\n",
    "\n",
    "    return input, o1, output\n",
    "\n",
    "\n",
    "def backforward(o0,o1,o2,t):\n",
    "    \"\"\"\n",
    "\n",
    "    :param o1:\n",
    "    :param o2:\n",
    "    :param t:\n",
    "    :param bias:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ## output layer:\n",
    "    # dE/do = (t-o)\n",
    "    diff = t-o2\n",
    "    dE_o2 = -diff\n",
    "    # do/dnet = o(1-o) for logistic function\n",
    "    dO_net =model.derivative[2](o2)\n",
    "\n",
    "    # dnet/w = o1\n",
    "    # delta = (t-o)*f'(net)= - dE/do * f'(net)\n",
    "    delta2 = -dE_o2 * dO_net\n",
    "\n",
    "    # hidden layer :\n",
    "    # Do not back-propagate bias in hidden layer,  to the last layer\n",
    "    # model.w2[0,:] is the bias vector between hidden layer and output layer\n",
    "    dE_o1 = -delta2 * model.w2[1:]\n",
    "    o1 = o1[1:]\n",
    "    dO1_net = model.derivative[1](o1)\n",
    "    delta1 = -dE_o1 * dO1_net\n",
    "\n",
    "    return delta1,delta2\n",
    "\n",
    "\n",
    "\n",
    "def predict(inputs,model):\n",
    "    pred = []\n",
    "    for i in inputs:\n",
    "        _,_, o= feedforward(i,model)\n",
    "        # print('o:',o)\n",
    "        pred.append(1 if o >= 0.5 else 0)\n",
    "\n",
    "    return  np.array(pred)\n",
    "\n",
    "\n",
    "def GDR_train(inputs,targets,model,bias=False,alpha0 = 0.5 , beta0 = 0.5,momentum= False,max_iter= 1000):\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs:  a set of input vectors, i has shape nxm,  n is the number of sample,  m is the size of one sample\n",
    "    in this project, each sample has 27 features, hence m=27\n",
    "\n",
    "    :param targets:   a list of target with shape 1xn\n",
    "    :param model:   Multi-layer neural network model, containing weights, bias and other parameters. See class MLNN\n",
    "    :param bias:    a flag indicating if bias should be used\n",
    "    :param momentum:\n",
    "    :param alpha:  learning rate\n",
    "    :param beta: weight of momentum\n",
    "    :param max_iter:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    err_tss= []\n",
    "    sensitivity = []\n",
    "    l = len(inputs)\n",
    "    delta_wn1 = np.zeros(np.shape(model.w1))\n",
    "    delta_wn2 = np.zeros(np.shape(model.w2))\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "        err =0.0\n",
    "        # update\n",
    "        alpha = alpha0*(max_iter-epoch)/max_iter\n",
    "        beta = beta0*(max_iter-epoch)/max_iter\n",
    "        # 1 epoch update\n",
    "        for i in range(l):\n",
    "            o0, o1, o2 =feedforward(inputs[i,:],model,bias)\n",
    "\n",
    "            diff =targets[i] - o2\n",
    "            err += np.square(diff)\n",
    "\n",
    "            # calculate Etss : Error = 0.5*(o-t)^2\n",
    "            # GDR update with backforward propagation\n",
    "            delta1, delta2 = backforward(o0,o1,o2,targets[i])\n",
    "\n",
    "            # update weights in layer 2\n",
    "            delta_w2 = alpha * delta2 * o1\n",
    "            model.w2 += delta_w2\n",
    "            # update weights in layer 1\n",
    "            delta_w1 = alpha*np.reshape(delta1,[len(delta1),1])*o0.reshape([1,len(o0)])\n",
    "            model.w1 += delta_w1\n",
    "\n",
    "            # when beta ==0, momentum is disabled\n",
    "            model.w2 += beta * delta_wn2\n",
    "            model.w1 += beta * delta_wn1\n",
    "            delta_wn1 = delta_w1\n",
    "            delta_wn2 = delta_w2\n",
    "\n",
    "        err= 0.5*err\n",
    "        if epoch%100 ==0:\n",
    "            err_tss.append(err)\n",
    "            # Test sensitivity here\n",
    "            tot_postive = np.sum(targets)\n",
    "            pred = predict(inputs, model)\n",
    "            TP = np.sum(pred[np.where(targets == 1)])\n",
    "            # print('targets:',targets)\n",
    "            # print('pred:',pred)\n",
    "            sens = TP / tot_postive\n",
    "            sensitivity.append(sens)\n",
    "            print(\"Error: \",err,\"Sensitivity:\",sens)\n",
    "\n",
    "    return  err_tss,sensitivity\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    from sklearn.utils import shuffle\n",
    "    t, x = load_data()\n",
    "    #normalize data\n",
    "    x = (x- np.min(x,axis=0))/(np.max(x,axis=0) - np.min(x,axis=0))\n",
    "    train_x = np.zeros([200,27])\n",
    "    train_x[0:100,:] = x[0:100,:]\n",
    "    train_x[100:, :] = x[200:300, :]\n",
    "    train_y=np.array([0]*200)\n",
    "    train_y[0:100] = t[0:100]\n",
    "    train_y[100:] = t[200:300]\n",
    "    i,t = shuffle(x,t)\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    print(train_x,train_y)\n",
    "\n",
    "    #balance data\n",
    "\n",
    "    # construct d x 2d+1 x 1 MFLL,  d= 27. Hence 3-layer network is 27x55x1\n",
    "    # The activation functions in the hidden layer are logistic function\n",
    "    model = MLFF(weight_initializer='random')\n",
    "\n",
    "    # o1, o = feedforward(i[1,:],model)\n",
    "    err_tss, sensitivity= GDR_train(i[0:500,:],t[0:500],model, bias=True,alpha0=1,beta0=0, momentum=False,max_iter=5000)\n",
    "    epochs = [i for i in range(len(err_tss))]\n",
    "    plt.plot(epochs,err_tss)\n",
    "    plt.plot(epochs, sensitivity)\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (my CI env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
