{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(over='ignore')\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)  # Seed the random number generator\n",
    "        self.weights = {}  # Create dict to hold weights\n",
    "        self.num_layers = 1  # Set initial number of layer to one (input layer)\n",
    "        self.adjustments = {}  # Create dict to hold adjustements\n",
    "\n",
    "    def add_layer(self, shape):\n",
    "        # Create weights with shape specified + biases\n",
    "        self.weights[self.num_layers] = np.vstack((2 * np.random.random(shape) - 1, 2 * np.random.random((1, shape[1])) - 1))\n",
    "        # Initialize the adjustements for these weights to zero\n",
    "        self.adjustments[self.num_layers] = np.zeros(shape)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # Pass data through pretrained network\n",
    "        for layer in range(1, self.num_layers+1):\n",
    "            data = np.dot(data, self.weights[layer-1][:, :-1]) + self.weights[layer-1][:, -1] # + self.biases[layer]\n",
    "            data = self.__sigmoid(data)\n",
    "        return data\n",
    "\n",
    "    def __forward_propagate(self, data):\n",
    "        # Progapagate through network and hold values for use in back-propagation\n",
    "        activation_values = {}\n",
    "        activation_values[1] = data\n",
    "        for layer in range(2, self.num_layers+1):\n",
    "            data = np.dot(data.T, self.weights[layer-1][:-1, :]) + self.weights[layer-1][-1, :].T # + self.biases[layer]\n",
    "            data = self.__sigmoid(data).T\n",
    "            activation_values[layer] = data\n",
    "        return activation_values\n",
    "\n",
    "    def simple_error(self, outputs, targets):\n",
    "        return targets - outputs\n",
    "\n",
    "    def sum_squared_error(self, outputs, targets):\n",
    "        return 0.5 * np.mean(np.sum(np.power(outputs - targets, 2), axis=1))\n",
    "\n",
    "    def __back_propagate(self, output, target):\n",
    "        deltas = {}\n",
    "        # Delta of output Layer\n",
    "        deltas[self.num_layers] = output[self.num_layers] - target\n",
    "\n",
    "        # Delta of hidden Layers\n",
    "        for layer in reversed(range(2, self.num_layers)):  # All layers except input/output\n",
    "            a_val = output[layer]\n",
    "            weights = self.weights[layer][:-1, :]\n",
    "            prev_deltas = deltas[layer+1]\n",
    "            deltas[layer] = np.multiply(np.dot(weights, prev_deltas), self.__sigmoid_derivative(a_val))\n",
    "\n",
    "        # Caclculate total adjustements based on deltas\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.adjustments[layer] += np.dot(deltas[layer+1], output[layer].T).T\n",
    "\n",
    "    def __gradient_descente(self, batch_size, learning_rate):\n",
    "        # Calculate partial derivative and take a step in that direction\n",
    "        for layer in range(1, self.num_layers):\n",
    "            partial_d = (1/batch_size) * self.adjustments[layer]\n",
    "            self.weights[layer][:-1, :] += learning_rate * -partial_d\n",
    "            self.weights[layer][-1, :] += learning_rate*1e-3 * -partial_d[-1, :]\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, num_epochs, learning_rate=1, stop_accuracy=1e-5):\n",
    "        error = []\n",
    "        \n",
    "        for iteration in range(num_epochs):\n",
    "            err = 0.0\n",
    "            for i in range(len(inputs)):\n",
    "                x = inputs[i]\n",
    "                y = targets[i]\n",
    "                # Pass the training set through our neural network\n",
    "                output = self.__forward_propagate(x)\n",
    "\n",
    "                # Calculate the error\n",
    "                loss = self.sum_squared_error(output[self.num_layers], y)\n",
    "                err +=loss\n",
    "                error.append(loss)\n",
    "\n",
    "                # Calculate Adjustements\n",
    "                self.__back_propagate(output, y)\n",
    "            \n",
    "            self.__gradient_descente(i, learning_rate)\n",
    "            if iteration%100 == 0:\n",
    "                print(\"sum error:\",err)\n",
    "            \n",
    "\n",
    "            # Check if accuarcy criterion is satisfied\n",
    "            if np.mean(error[-(i+1):]) < stop_accuracy and iteration > 0:\n",
    "                break\n",
    "\n",
    "        return(np.asarray(error), iteration+1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def balance_data(aep_i,aep_t, non_aep_i,non_aep_t,method ='downsampling', test_aep_sample= 30,test_nonaep_sample= 1200):\n",
    "    \"\"\"\n",
    "\n",
    "    :param aep_i:\n",
    "    :param aep_t:\n",
    "    :param non_aep_i:\n",
    "    :param non_aep_t:\n",
    "    :param method:\n",
    "    :param test_aep_sample:\n",
    "    :param test_nonaep_sample:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # split test set and train set\n",
    "    aep_test_i = aep_i[0:test_aep_sample,:]\n",
    "    aep_test_t = aep_t[0:test_aep_sample]\n",
    "    non_aep_test_i = non_aep_i[0:test_nonaep_sample,:]\n",
    "    non_aep_test_t = non_aep_t[0:test_nonaep_sample]\n",
    "\n",
    "    # test set\n",
    "    test_i = np.concatenate((aep_test_i,non_aep_test_i),axis=0)\n",
    "    test_t = []\n",
    "    test_t.extend(aep_test_t)\n",
    "    test_t.extend( non_aep_test_t)\n",
    "\n",
    "    # train set\n",
    "    aep_i =aep_i[test_aep_sample:,:]\n",
    "    aep_t = np.array(aep_t[test_aep_sample:])\n",
    "    non_aep_i = non_aep_i[test_nonaep_sample:, :]\n",
    "    non_aep_t = non_aep_t[test_nonaep_sample:]\n",
    "\n",
    " \n",
    "    # balance training set here\n",
    "    if method == 'oversampling':\n",
    "        sample_range = len(aep_i)\n",
    "        indices = np.random.randint(sample_range, size=len(non_aep_i))\n",
    "        new_aep_i = aep_i[indices,:]\n",
    "        new_aep_t = aep_t[indices]\n",
    "        \n",
    "        train_i = np.concatenate((new_aep_i, non_aep_i),axis=0)\n",
    "        train_t =[]\n",
    "        train_t.extend(new_aep_t)\n",
    "        train_t.extend(non_aep_t)\n",
    "\n",
    "\n",
    "    elif method == 'downsampling':\n",
    "        sample_range = len(non_aep_i)\n",
    "        indices = np.random.randint(sample_range, size=len(aep_i))\n",
    "        new_nonaep_i = non_aep_i[indices,:]\n",
    "        new_nonaep_t = non_aep_t[indices]\n",
    "        \n",
    "        train_i = np.concatenate((aep_i,new_nonaep_i),axis=0)\n",
    "        train_t =[]\n",
    "        train_t.extend(aep_t)\n",
    "        train_t.extend(new_nonaep_t)\n",
    "        pass\n",
    "    else:\n",
    "        sample_size = 500\n",
    "        \n",
    "        indices = np.random.randint(len(non_aep_i), size=sample_size)\n",
    "        new_nonaep_i = non_aep_i[indices,:]\n",
    "        new_nonaep_t = non_aep_t[indices]\n",
    "        \n",
    "        indices = np.random.randint(len(aep_i), size=sample_size)\n",
    "        new_aep_i = aep_i[indices,:]\n",
    "        new_aep_t = aep_t[indices]\n",
    "        \n",
    "        train_i = np.concatenate((new_aep_i,new_nonaep_i),axis=0)\n",
    "        train_t =[]\n",
    "        train_t.extend(new_aep_t)\n",
    "        train_t.extend(new_nonaep_t)\n",
    "        pass\n",
    "\n",
    "\n",
    "    return train_i,train_t,test_i,test_t\n",
    "\n",
    "\n",
    "\n",
    "def load_balanced_data():\n",
    "    fp = open('db4_Diff1f_100_128w_AEP2_gp2_ver1-1_ece856.txt', 'r+')\n",
    "    lines = fp.readlines()\n",
    "    aep_i = []\n",
    "    aep_t = []\n",
    "    non_aep_i = []\n",
    "    non_aep_t = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.split('\\n')[0]\n",
    "        data = [float(s) for s in line.split(' ') if s != '']\n",
    "        aep_i.append(data)\n",
    "        aep_t.append(1)\n",
    "    fp.close()\n",
    "\n",
    "    aep_i = np.array(aep_i)\n",
    "    aep_t = np.array(aep_t)\n",
    "\n",
    "    fp = open('db4_Diff1f_100_128w_nonAEP2_gp2_ver1-1_ece856.txt', 'r+')\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.split('\\n')[0]\n",
    "        data = [float(s) for s in line.split(' ') if s != '']\n",
    "        non_aep_i.append(data)\n",
    "        non_aep_t.append(0)\n",
    "    fp.close()\n",
    "    non_aep_i = np.array(non_aep_i)\n",
    "    non_aep_t = np.array(non_aep_t)\n",
    "\n",
    "    train_i, train_t, test_i, test_t= balance_data(aep_i,aep_t,non_aep_i,non_aep_t,method='')\n",
    "#     save_balanced_data(train_i, train_t, test_i, test_t)\n",
    "\n",
    "    return train_i, train_t, test_i, test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1000, 27, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ----------- XOR Function -----------------\n",
    "# Create instance of a neural network\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Add Layers (Input layer is created by default)\n",
    "nn.add_layer((27, 27*2+1))\n",
    "nn.add_layer((27*2+1, 1))\n",
    "\n",
    "# XOR function\n",
    "train_i, train_t, test_i, test_t = load_balanced_data()\n",
    "from sklearn.utils import shuffle\n",
    "train_i, train_t = shuffle(train_i, train_t)\n",
    "training_data = np.reshape(train_i,(len(train_i), 27, 1))\n",
    "training_labels = np.array(train_t).reshape((len(train_t),1))\n",
    "print(\"shape:\", training_data.shape)\n",
    "\n",
    "    # nn.predict(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum error: 210.54419567014955\n",
      "sum error: 250.0\n",
      "sum error: 249.99999999861592\n",
      "sum error: 250.0\n",
      "sum error: 244.0\n",
      "sum error: 244.0\n",
      "sum error: 244.0\n",
      "sum error: 244.0\n",
      "sum error: 244.0\n",
      "sum error: 250.0\n",
      "sum error: 244.0\n",
      "sum error: 250.0\n",
      "sum error: 243.5\n",
      "sum error: 250.0\n",
      "sum error: 244.0\n",
      "sum error: 244.0\n",
      "sum error: 244.0\n",
      "sum error: 250.0\n",
      "sum error: 250.0\n",
      "sum error: 250.0\n",
      "sum error: 250.0\n",
      "sum error: 243.5\n",
      "sum error: 243.5\n",
      "sum error: 243.4993872128567\n",
      "sum error: 250.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e4c00d47a7cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoches needed to train = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-282423574e06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, targets, num_epochs, learning_rate, stop_accuracy)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# Calculate the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-282423574e06>\u001b[0m in \u001b[0;36msum_squared_error\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msum_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__back_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2920\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "error,iteration = nn.train(training_data, training_labels, 8000)\n",
    "print('Error = ', np.mean(error[-4:]))\n",
    "print('Epoches needed to train = ', iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (my CI env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
